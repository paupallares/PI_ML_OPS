{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo las librerias necesarias\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/dataset_limpio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>['Animation', 'Comedy', 'Family']</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td></td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>['Adventure', 'Fantasy', 'Family']</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>['English', 'Français']</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>['Romance', 'Comedy']</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance']</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title                              genres   \n",
       "0    862                    Toy Story   ['Animation', 'Comedy', 'Family']  \\\n",
       "1   8844                      Jumanji  ['Adventure', 'Fantasy', 'Family']   \n",
       "2  15602             Grumpier Old Men               ['Romance', 'Comedy']   \n",
       "3  31357            Waiting to Exhale      ['Comedy', 'Drama', 'Romance']   \n",
       "4  11862  Father of the Bride Part II                          ['Comedy']   \n",
       "\n",
       "                                            overview   \n",
       "0  Led by Woody, Andy's toys live happily in his ...  \\\n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                                             tagline         spoken_languages   \n",
       "0                                                                 ['English']  \\\n",
       "1          Roll the dice and unleash the excitement!  ['English', 'Français']   \n",
       "2  Still Yelling. Still Fighting. Still Ready for...              ['English']   \n",
       "3  Friends are the people who let you be yourself...              ['English']   \n",
       "4  Just When His World Is Back To Normal... He's ...              ['English']   \n",
       "\n",
       "  decade  \n",
       "0   1990  \n",
       "1   1990  \n",
       "2   1990  \n",
       "3   1990  \n",
       "4   1990  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chequeo que se importaron bien los datos\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42196 entries, 0 to 42195\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   id                42196 non-null  object\n",
      " 1   title             42196 non-null  object\n",
      " 2   genres            42196 non-null  object\n",
      " 3   overview          42196 non-null  object\n",
      " 4   tagline           42196 non-null  object\n",
      " 5   spoken_languages  42196 non-null  object\n",
      " 6   decade            42196 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                  0\n",
       "title               0\n",
       "genres              0\n",
       "overview            0\n",
       "tagline             0\n",
       "spoken_languages    0\n",
       "decade              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la idea principal es crear dos matrices, una para 'overview' y 'tagline' calculando la simiitud usando la matriz TF-IDF ya que este metodo pondera la importancia de cada palabra en relacion a todo el texto, lo que captura mejor las palabras claves o terminos especificos. \n",
    "para 'genres', 'decade' y 'spoken_languages' usar una matriz de conteo ya que esta simplemente cuenta la frecuencia de cada palabra sin tener en cuenta el resto del texto, considerando que estos datos son más cercanos a un metadato que a datos de lenguaje como si ocurre en 'overview' y 'tagline'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a empezar con la matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminación de caracteres especiales y conversión a minúsculas:\n",
    "df['overview'] = df['overview'].str.replace('[^\\w\\s]', '').str.lower().str.replace(\",\", \"\").str.replace(\".\", \"\").str.replace(\"!\", \"\")\n",
    "df['tagline'] = df['tagline'].str.replace(\"[^\\w\\s]\", \"\").str.lower().str.replace(\",\", \"\").str.replace(\".\", \"\").str.replace(\"!\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ssanjua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ssanjua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/4rl6w509079c20dtbvnqwf1c0000gn/T/ipykernel_9169/2198163012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4624\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4625\u001b[0m         \"\"\"\n\u001b[0;32m-> 4626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4628\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/var/folders/71/4rl6w509079c20dtbvnqwf1c0000gn/T/ipykernel_9169/2198163012.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/71/4rl6w509079c20dtbvnqwf1c0000gn/T/ipykernel_9169/2198163012.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overview'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tagline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# 3. If there are no matches, keep applying rules until we find a match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m             \u001b[0mforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mapply_rules\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_rules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m   2013\u001b[0m                 \u001b[0mform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2014\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubstitutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m             ]\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#eliminación de palabras vacías (stopwords) y lematización:\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['overview'] = df['overview'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))\n",
    "df['tagline'] = df['tagline'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"led woody andy's toy live happily room andy's birthday brings buzz lightyear onto scene afraid losing place andy's heart woody plot buzz circumstance separate buzz woody owner duo eventually learns put aside difference\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roll dice unleash excitement'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tagline'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df[['overview', 'tagline']]\n",
    "#combinamos los campos en un solo texto\n",
    "df['texto_combinado'] = text_data['overview'] + ' ' + text_data['tagline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminamos las columnas ya que no las necesitaremos mas\n",
    "df = df.drop('overview', axis=1)\n",
    "df = df.drop('tagline', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>decade</th>\n",
       "      <th>texto_combinado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>['Animation', 'Comedy', 'Family']</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "      <td>led by woody andy's toys live happily in his r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>['Adventure', 'Fantasy', 'Family']</td>\n",
       "      <td>['English', 'Français']</td>\n",
       "      <td>1990</td>\n",
       "      <td>when siblings judy and peter discover an encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>['Romance', 'Comedy']</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "      <td>a family wedding reignites the ancient feud be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance']</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "      <td>cheated on mistreated and stepped on the women...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>['English']</td>\n",
       "      <td>1990</td>\n",
       "      <td>just when george banks has recovered from his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title                              genres   \n",
       "0    862                    Toy Story   ['Animation', 'Comedy', 'Family']  \\\n",
       "1   8844                      Jumanji  ['Adventure', 'Fantasy', 'Family']   \n",
       "2  15602             Grumpier Old Men               ['Romance', 'Comedy']   \n",
       "3  31357            Waiting to Exhale      ['Comedy', 'Drama', 'Romance']   \n",
       "4  11862  Father of the Bride Part II                          ['Comedy']   \n",
       "\n",
       "          spoken_languages decade   \n",
       "0              ['English']   1990  \\\n",
       "1  ['English', 'Français']   1990   \n",
       "2              ['English']   1990   \n",
       "3              ['English']   1990   \n",
       "4              ['English']   1990   \n",
       "\n",
       "                                     texto_combinado  \n",
       "0  led by woody andy's toys live happily in his r...  \n",
       "1  when siblings judy and peter discover an encha...  \n",
       "2  a family wedding reignites the ancient feud be...  \n",
       "3  cheated on mistreated and stepped on the women...  \n",
       "4  just when george banks has recovered from his ...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el vectorizador TF-IDF. lo limito a 50000 para que no sea tan grande\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', max_features=50000)\n",
    "\n",
    "# Aplicar el vectorizador a los datos de texto\n",
    "tfidf_matrix = vectorizer.fit_transform(df['texto_combinado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42196, 50000)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similaritud del coseno: \n",
    "coseno_sim_text = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "#toma entre 35s a 50s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNT MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quito los espacios en los generos formados por mas de una silaba\n",
    "df['genres'] = df['genres'].str.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplico 'genres' para darle mas importancia y las uno en un campo\n",
    "df['combined_features'] = df['genres'] + df['genres'] + df['decade'] + df['spoken_languages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Action','Adventure','Drama','Family']['Action','Adventure','Drama','Family']1990['English', 'Deutsch']\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined_features'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elimino las columnas que ya no me sirven\n",
    "df = df.drop('genres', axis=1)\n",
    "df = df.drop('decade', axis=1)\n",
    "df = df.drop('id', axis=1)\n",
    "df = df.drop('spoken_languages', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_features'] = df['combined_features'].str.replace(\"'\", \" \").str.replace(\"[\", \" \").str.replace(\"]\", \" \").str.replace(\",\", \" \").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>texto_combinado</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>led by woody andy's toys live happily in his r...</td>\n",
       "      <td>animation   comedy   family    animation   c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>when siblings judy and peter discover an encha...</td>\n",
       "      <td>adventure   fantasy   family    adventure   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>a family wedding reignites the ancient feud be...</td>\n",
       "      <td>romance   comedy    romance   comedy  1990  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>cheated on mistreated and stepped on the women...</td>\n",
       "      <td>comedy   drama   romance    comedy   drama  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>just when george banks has recovered from his ...</td>\n",
       "      <td>comedy    comedy  1990  english</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title   \n",
       "0                    Toy Story  \\\n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                     texto_combinado   \n",
       "0  led by woody andy's toys live happily in his r...  \\\n",
       "1  when siblings judy and peter discover an encha...   \n",
       "2  a family wedding reignites the ancient feud be...   \n",
       "3  cheated on mistreated and stepped on the women...   \n",
       "4  just when george banks has recovered from his ...   \n",
       "\n",
       "                                   combined_features  \n",
       "0    animation   comedy   family    animation   c...  \n",
       "1    adventure   fantasy   family    adventure   ...  \n",
       "2    romance   comedy    romance   comedy  1990  ...  \n",
       "3    comedy   drama   romance    comedy   drama  ...  \n",
       "4                  comedy    comedy  1990  english    "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorización de texto\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "feature_matrix = vectorizer.fit_transform(df['combined_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42196, 246)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "coseno_sim_features = cosine_similarity(feature_matrix)\n",
    "\n",
    "#tarda mas de 1min y medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina las similitudes utilizando algún método (por ejemplo, suma ponderada)\n",
    "combined_similarity = 0.6 * coseno_sim_text + 0.4 * coseno_sim_features\n",
    "\n",
    "#tarda mas de 5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    titles = df['title']\n",
    "    indices = pd.Series(df.index, index=df['title'])  \n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(combined_similarity[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 20 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2961                Toy Story 2\n",
       "14764               Toy Story 3\n",
       "23200                 Small Fry\n",
       "20859    Bartok the Magnificent\n",
       "726               A Close Shave\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20145    Despicable Me 2\n",
       "5271     Stuart Little 2\n",
       "40892    Despicable Me 3\n",
       "18828     Wreck-It Ralph\n",
       "32789          Dino Time\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Minions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6320     Lara Croft Tomb Raider: The Cradle of Life\n",
       "25040                                Thor: Ragnarok\n",
       "9904                                 Fantastic Four\n",
       "14579                                  Solomon Kane\n",
       "5775                               Heavy Metal 2000\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Avatar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2045      Return to Paradise\n",
       "342              Bitter Moon\n",
       "652                     Fear\n",
       "9255    When Will I Be Loved\n",
       "1418                   Crash\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2502                          Superman II\n",
       "10733                    Superman Returns\n",
       "2504     Superman IV: The Quest for Peace\n",
       "30221                Atom Man vs Superman\n",
       "20058                        Man of Steel\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('Superman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42196, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el modelo funciona bastante bien hasta acá, pero demora demasiado en ejecutarse por lo que desde ya voy a abandonar la idea de aplicar un filtro para que considere las puntuaciones y la popularidad porque va a seguir sumando carga al modelo.\n",
    "\n",
    "para poder deployar y hacerlo funcionar en tiempo razonable voy a tener que reducir el tamaño de mi set de prueba, intentando seguir con las dos matrices pero entendiendo que lo que más tiempo le lleva al modelo es encontrarla similaritud entre ambas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo un nuevo dataset mas pequeño\n",
    "small_df = df.sample(n=8000, random_state=42)\n",
    "\n",
    "#le reseteo el indice para evitar problemas\n",
    "small_df=small_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>texto_combinado</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Business of Fancydancing</td>\n",
       "      <td>seymour polatkin is a successful gay indian po...</td>\n",
       "      <td>music   drama    music   drama  2000  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollman</td>\n",
       "      <td>brick bardo is a traveller from outer space wh...</td>\n",
       "      <td>action   comedy   crime   sciencefiction    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dukes</td>\n",
       "      <td>the dukesa doo wop group were on top of the wo...</td>\n",
       "      <td>comedy   crime   drama   music    comedy   c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Wars: D-War</td>\n",
       "      <td>based on the korean legend unknown creatures w...</td>\n",
       "      <td>fantasy   drama   horror   action   thriller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom and Huck</td>\n",
       "      <td>a mischievous young boy tom sawyer witnesses a...</td>\n",
       "      <td>action   adventure   drama   family    actio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>Surviving Sid</td>\n",
       "      <td>sid the sloth takes a school of children out o...</td>\n",
       "      <td>animation    animation  2000  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>The Devil's Ground</td>\n",
       "      <td>while traveling from california to bangor thro...</td>\n",
       "      <td>horror   thriller    horror   thriller  2000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>Keep Your Right Up</td>\n",
       "      <td>this film is made up several sketches in which...</td>\n",
       "      <td>comedy   drama    comedy   drama  1980  fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>Mac</td>\n",
       "      <td>niccolo (mac) vitelli is the eldest of three b...</td>\n",
       "      <td>drama    drama  1990  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>Waiting for Good News</td>\n",
       "      <td>general store manager akio (tetsuji tamayama) ...</td>\n",
       "      <td>drama    drama  2000  日本語</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title   \n",
       "0     The Business of Fancydancing  \\\n",
       "1                          Dollman   \n",
       "2                        The Dukes   \n",
       "3               Dragon Wars: D-War   \n",
       "4                     Tom and Huck   \n",
       "...                            ...   \n",
       "7995                 Surviving Sid   \n",
       "7996            The Devil's Ground   \n",
       "7997            Keep Your Right Up   \n",
       "7998                           Mac   \n",
       "7999         Waiting for Good News   \n",
       "\n",
       "                                        texto_combinado   \n",
       "0     seymour polatkin is a successful gay indian po...  \\\n",
       "1     brick bardo is a traveller from outer space wh...   \n",
       "2     the dukesa doo wop group were on top of the wo...   \n",
       "3     based on the korean legend unknown creatures w...   \n",
       "4     a mischievous young boy tom sawyer witnesses a...   \n",
       "...                                                 ...   \n",
       "7995  sid the sloth takes a school of children out o...   \n",
       "7996  while traveling from california to bangor thro...   \n",
       "7997  this film is made up several sketches in which...   \n",
       "7998  niccolo (mac) vitelli is the eldest of three b...   \n",
       "7999  general store manager akio (tetsuji tamayama) ...   \n",
       "\n",
       "                                      combined_features  \n",
       "0       music   drama    music   drama  2000  english    \n",
       "1       action   comedy   crime   sciencefiction    ...  \n",
       "2       comedy   crime   drama   music    comedy   c...  \n",
       "3       fantasy   drama   horror   action   thriller...  \n",
       "4       action   adventure   drama   family    actio...  \n",
       "...                                                 ...  \n",
       "7995            animation    animation  2000  english    \n",
       "7996    horror   thriller    horror   thriller  2000...  \n",
       "7997    comedy   drama    comedy   drama  1980  fran...  \n",
       "7998                    drama    drama  1990  english    \n",
       "7999                        drama    drama  2000  日本語    \n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 32877)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el vectorizador TF-IDF. lo limito a 50000 para que no sea tan grande\n",
    "vectorizerSML = TfidfVectorizer(analyzer='word', stop_words='english', max_features=50000)\n",
    "\n",
    "# Aplicar el vectorizador a los datos de texto\n",
    "tfidf_matrixSML = vectorizerSML.fit_transform(small_df['texto_combinado'])\n",
    "\n",
    "tfidf_matrixSML.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similaritud del coseno del TF-IDF: \n",
    "coseno_sim_textdSML = cosine_similarity(tfidf_matrixSML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 167)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conteo de vectores con los features\n",
    "vectorizerSML_features = CountVectorizer(stop_words='english')\n",
    "feature_matrixSML = vectorizerSML_features.fit_transform(small_df['combined_features'])\n",
    "\n",
    "feature_matrixSML.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similaritud del coseno\n",
    "coseno_sim_featuresSML = cosine_similarity(feature_matrixSML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina las similitudes utilizando algún método (por ejemplo, suma ponderada)\n",
    "combined_similaritySML = 0.6 * coseno_sim_textdSML + 0.4 * coseno_sim_featuresSML\n",
    "\n",
    "#demora menos de 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendationsSML(title):\n",
    "    titles = small_df['title']\n",
    "    indices = pd.Series(small_df.index, index=small_df['title'])  \n",
    "    if title not in indices:\n",
    "        return \"La película no está en el dataset reducido\"\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(combined_similaritySML[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 5 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6988               Love at Large\n",
       "563            Little Big League\n",
       "7384    Smiles of a Summer Night\n",
       "7141                     Godsend\n",
       "2982        The James Dean Story\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendationsSML('The Dukes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La película no está en el dataset reducido'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendationsSML('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicion final para API:\n",
    "def recomendacion(title:str):\n",
    "    indices = small_df[small_df['title'] == title]\n",
    "    if indices.empty:\n",
    "        return \"La película no está en el dataset reducido\"\n",
    "\n",
    "    idx = indices.index[0]\n",
    "    vectorizerTF = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    tfidf_matrixTF = vectorizerTF.fit_transform(small_df['texto_combinado'])\n",
    "    coseno_sim_text = cosine_similarity(tfidf_matrixTF)\n",
    "\n",
    "    vectorizer_features = CountVectorizer(stop_words='english')\n",
    "    feature_matrix = vectorizer_features.fit_transform(small_df['combined_features'])\n",
    "    coseno_sim_features = cosine_similarity(feature_matrix)\n",
    "\n",
    "    combined_similarity = 0.6 * coseno_sim_text + 0.4 * coseno_sim_features\n",
    "\n",
    "    sim_scores = list(enumerate(combined_similarity[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 5 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    movie_titles = [small_df['title'].iloc[i].title() for i in movie_indices]\n",
    "\n",
    "    \n",
    "    return {'lista recomendada': movie_titles} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La película no está en el dataset reducido'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacion('Minion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La película no está en el dataset reducido'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacion('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lista recomendada': [\"Mini'S First Time\",\n",
       "  'Girltrash: All Night Long',\n",
       "  'Taking Woodstock',\n",
       "  'Duets',\n",
       "  'Little Fish, Strange Pond']}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacion('The Dukes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_df.to_csv('Datasets/dataset_reducido.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = pd.read_csv('Datasets/dataset_reducido.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendacion2(title:str):\n",
    "    '''Ingresas un nombre de pelicula y te recomienda las similares en una lista'''\n",
    "    #small_df = pd.read_csv('../Datasets/dataset_reducido.csv')\n",
    "    indices = pd.Series(df.index, index=df['title'])  \n",
    "    if title not in indices:\n",
    "        return \"La película no está en el dataset reducido\"\n",
    "    idx = indices[title]\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(small_df['texto_combinado'])\n",
    "    coseno_sim_text = cosine_similarity(tfidf_matrix)\n",
    "    vectorizer_features = CountVectorizer(stop_words='english')\n",
    "    feature_matrix = vectorizer_features.fit_transform(small_df['combined_features'])\n",
    "    coseno_sim_features = cosine_similarity(feature_matrix)\n",
    "    combined_similarity = 0.6 * coseno_sim_text + 0.4 * coseno_sim_features\n",
    "\n",
    "    sim_scores = list(enumerate(combined_similarity[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 5 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    recommendations=list(small_df['title'].iloc[movie_indices].str.title())\n",
    "    \n",
    "    return {'lista recomendada': recommendations} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lista recomendada': ['Hendrix',\n",
       "  'Masked And Anonymous',\n",
       "  'Passing Strange',\n",
       "  'Madame Sousatzka',\n",
       "  'Winter Solstice']}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacion2('Toy Story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>texto_combinado</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Business of Fancydancing</td>\n",
       "      <td>seymour polatkin successful gay indian poet sp...</td>\n",
       "      <td>music   drama    music   drama  2000  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollman</td>\n",
       "      <td>brick bardo traveller outer space forced land ...</td>\n",
       "      <td>action   comedy   crime   sciencefiction    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dukes</td>\n",
       "      <td>dukesa doo wop group top world 17 struggling s...</td>\n",
       "      <td>comedy   crime   drama   music    comedy   c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Wars: D-War</td>\n",
       "      <td>based korean legend unknown creature return de...</td>\n",
       "      <td>fantasy   drama   horror   action   thriller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tom and Huck</td>\n",
       "      <td>mischievous young boy tom sawyer witness murde...</td>\n",
       "      <td>action   adventure   drama   family    actio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Slaughterhouse</td>\n",
       "      <td>owner slaughterhouse facing foreclosure instru...</td>\n",
       "      <td>horror    horror  1980  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Mother's Heart</td>\n",
       "      <td>lorenz three young child victim medium wish tu...</td>\n",
       "      <td>drama    drama  1960  italiano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Antisocial</td>\n",
       "      <td>five university friend gather house party ring...</td>\n",
       "      <td>horror   thriller    horror   thriller  2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Shadows of the Dead</td>\n",
       "      <td>group teenager try escape creature life among ...</td>\n",
       "      <td>horror    horror  2010  english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Me and Morrison</td>\n",
       "      <td>bittersweet tale love affair two people whose ...</td>\n",
       "      <td>romance    romance  2000  suomi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title   \n",
       "0     The Business of Fancydancing  \\\n",
       "1                          Dollman   \n",
       "2                        The Dukes   \n",
       "3               Dragon Wars: D-War   \n",
       "4                     Tom and Huck   \n",
       "...                            ...   \n",
       "9995                Slaughterhouse   \n",
       "9996                Mother's Heart   \n",
       "9997                    Antisocial   \n",
       "9998           Shadows of the Dead   \n",
       "9999               Me and Morrison   \n",
       "\n",
       "                                        texto_combinado   \n",
       "0     seymour polatkin successful gay indian poet sp...  \\\n",
       "1     brick bardo traveller outer space forced land ...   \n",
       "2     dukesa doo wop group top world 17 struggling s...   \n",
       "3     based korean legend unknown creature return de...   \n",
       "4     mischievous young boy tom sawyer witness murde...   \n",
       "...                                                 ...   \n",
       "9995  owner slaughterhouse facing foreclosure instru...   \n",
       "9996  lorenz three young child victim medium wish tu...   \n",
       "9997  five university friend gather house party ring...   \n",
       "9998  group teenager try escape creature life among ...   \n",
       "9999  bittersweet tale love affair two people whose ...   \n",
       "\n",
       "                                      combined_features  \n",
       "0       music   drama    music   drama  2000  english    \n",
       "1       action   comedy   crime   sciencefiction    ...  \n",
       "2       comedy   crime   drama   music    comedy   c...  \n",
       "3       fantasy   drama   horror   action   thriller...  \n",
       "4       action   adventure   drama   family    actio...  \n",
       "...                                                 ...  \n",
       "9995                  horror    horror  1980  english    \n",
       "9996                   drama    drama  1960  italiano    \n",
       "9997    horror   thriller    horror   thriller  2010...  \n",
       "9998                  horror    horror  2010  english    \n",
       "9999                  romance    romance  2000  suomi    \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['text'] = small_df['texto_combinado'] + small_df['combined_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_df = small_df[['title', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_df.to_csv('Datasets/dataset_tiny.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendacionTINY(titulo:str):\n",
    "    '''Ingresas un nombre de pelicula y te recomienda las similares en una lista'''\n",
    "    indices = tiny_df[tiny_df['title'] == titulo]\n",
    "    if indices.empty:\n",
    "        return \"La película no está en el dataset reducido\"\n",
    "    idx = indices.index[0]\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(tiny_df['text'])\n",
    "    coseno_sim = cosine_similarity(tfidf_matrix)\n",
    "    sim_scores = list(enumerate(coseno_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 5 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    recommendations = list(tiny_df['title'].iloc[movie_indices].str.title())\n",
    "    \n",
    "    return {'lista recomendada': recommendations} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lista recomendada': ['Beautiful Vera',\n",
       "  'Get Yourself A College Girl',\n",
       "  'God Save The King',\n",
       "  'I Want You',\n",
       "  'Rock On 2']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomendacionTINY('The Dukes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente etapa implica la construcción de una matriz de similitud entre las películas. Esta matriz nos ayudará a calcular la similitud entre películas y así generar recomendaciones basadas en películas similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get('/recomendacion/{titulo}')\n",
    "def recomendacion(titulo:str):\n",
    "    '''Ingresas un nombre de pelicula y te recomienda las similares en una lista'''\n",
    "    indices = small_df[small_df['title'] == titulo]\n",
    "    if indices.empty:\n",
    "        return \"La película no está en el dataset reducido\"\n",
    "    idx = indices.index[0]\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(small_df['texto_combinado'])\n",
    "    coseno_sim_text = cosine_similarity(tfidf_matrix)\n",
    "    vectorizer_features = CountVectorizer(stop_words='english')\n",
    "    feature_matrix = vectorizer_features.fit_transform(small_df['combined_features'])\n",
    "    coseno_sim_features = cosine_similarity(feature_matrix)\n",
    "    combined_similarity = 0.6 * coseno_sim_text + 0.4 * coseno_sim_features\n",
    "\n",
    "    sim_scores = list(enumerate(combined_similarity[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # Obtenemos las 5 películas más similares\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    recommendations=list(small_df['title'].iloc[movie_indices].str.title())\n",
    "    \n",
    "    return {'lista recomendada': recommendations} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
